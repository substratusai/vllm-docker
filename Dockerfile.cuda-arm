# Source: https://github.com/LambdaLabsML/vllm-builder
# Had to switch to 12.4.1 because 12.6.3 was causing Segmentation fault when vLLM builds flash-attn
ARG CUDA_VERSION=12.4.1
ARG IMAGE_DISTRO=ubuntu22.04
ARG PYTHON_VERSION=3.12

# ---------- Builder Base ----------
FROM nvcr.io/nvidia/cuda:${CUDA_VERSION}-devel-${IMAGE_DISTRO} AS base

ARG TORCH_CUDA_ARCH_LIST="9.0a"
ENV TORCH_CUDA_ARCH_LIST=${TORCH_CUDA_ARCH_LIST}
ARG VLLM_FA_CMAKE_GPU_ARCHES="90a-real"
ENV VLLM_FA_CMAKE_GPU_ARCHES=${VLLM_FA_CMAKE_GPU_ARCHES}
ENV UV_HTTP_TIMEOUT=500

# Update apt packages and install dependencies
ENV DEBIAN_FRONTEND=noninteractive
RUN apt update
RUN apt upgrade -y
RUN apt install -y --no-install-recommends \
        curl \
        git \
        libibverbs-dev \
        zlib1g-dev

# Clean apt cache
RUN apt clean
RUN rm -rf /var/lib/apt/lists/*
RUN rm -rf /var/cache/apt/archives

# Set compiler paths
ENV CC=/usr/bin/gcc
ENV CXX=/usr/bin/g++

# Install uv
RUN curl -LsSf https://astral.sh/uv/install.sh | env UV_INSTALL_DIR=/usr/local/bin sh

# Setup build workspace
WORKDIR /workspace

# Prep build venv
ARG PYTHON_VERSION
RUN uv venv -p ${PYTHON_VERSION} --seed --python-preference only-managed
ENV VIRTUAL_ENV=/workspace/.venv
ENV PATH=${VIRTUAL_ENV}/bin:${PATH}
ENV CUDA_HOME=/usr/local/cuda
ENV LD_LIBRARY_PATH=${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}

# Install pytorch nightly
RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu126

FROM base AS build-base
RUN mkdir /wheels

# Install build deps that aren't in project requirements files
# Make sure to upgrade setuptools to avoid triton build bug
RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install -U build cmake ninja pybind11 setuptools wheel

FROM build-base AS build-triton
ARG TRITON_REF=release/3.2.x
ARG TRITON_BUILD_SUFFIX=+cu126
ENV TRITON_WHEEL_VERSION_SUFFIX=${TRITON_BUILD_SUFFIX:-}
RUN git clone https://github.com/triton-lang/triton.git
RUN cd triton && \
    git checkout ${TRITON_REF} && \
    git submodule sync && \
    git submodule update --init --recursive -j 8 && \
    # Unclear why this is needed, makes no sense, but without it kept on hitting Connection timeout downloading the tar
    curl -O https://anaconda.org/nvidia/cuda-nvcc/12.4.99/download/linux-aarch64/cuda-nvcc-12.4.99-0.tar.bz2 && \
    uv build python --wheel --no-build-isolation -o /wheels

FROM build-base AS build-xformers
ARG XFORMERS_REF=v0.0.29.post2
ARG XFORMERS_BUILD_VERSION=0.0.29.post2+cu126
ENV BUILD_VERSION=${XFORMERS_BUILD_VERSION:-${XFORMERS_REF#v}}
RUN git clone  https://github.com/facebookresearch/xformers.git
RUN cd xformers && \
    git checkout ${XFORMERS_REF} && \
    git submodule sync && \
    git submodule update --init --recursive -j 8 && \
    uv build --wheel --no-build-isolation -o /wheels

FROM build-base AS build-flashinfer
ARG FLASHINFER_ENABLE_AOT=1
ARG FLASHINFER_REF=v0.2.2.post1
ARG FLASHINFER_BUILD_SUFFIX=cu126
ENV FLASHINFER_LOCAL_VERSION=${FLASHINFER_BUILD_SUFFIX:-}
# Flashinfer only supports sm75+, removing 7.0 from arch list
# ENV TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'
RUN git clone https://github.com/flashinfer-ai/flashinfer.git
RUN cd flashinfer && \
    git checkout ${FLASHINFER_REF} && \
    git submodule sync && \
    git submodule update --init --recursive -j 8 && \
    NVCC_THREADS=2 \
    MAX_JOBS=32 \
    uv build --wheel --no-build-isolation -o /wheels
# Restore original CUDA arch list for subsequent stages
ENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}

FROM build-base AS build-vllm
ARG VLLM_REF=v0.8.2
RUN git clone https://github.com/vllm-project/vllm.git
RUN cd vllm && \
    git checkout ${VLLM_REF} && \
    git submodule sync && \
    git submodule update --init --recursive -j 8 && \
    uv pip install -r requirements/build.txt && \
    NVCC_THREADS=2 \
    MAX_JOBS=32 \
    uv build --wheel --no-build-isolation -o /wheels

FROM base AS vllm-openai
COPY --from=build-flashinfer /wheels/* wheels/
COPY --from=build-triton /wheels/* wheels/
COPY --from=build-vllm /wheels/* wheels/
COPY --from=build-xformers /wheels/* wheels/

# Install triton using upstream wheel
# RUN --mount=type=cache,target=/root/.cache/uv \
#     uv pip install --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40;

# Install and cleanup wheels
RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install wheels/*
RUN rm -r wheels

# Install pynvml
RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install pynvml

# Add additional packages for vLLM OpenAI
RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install accelerate hf_transfer 'modelscope!=1.15.0' 'timm==0.9.10' bitsandbytes boto3 runai-model-streamer runai-model-streamer[s3] tensorizer

# Clean uv cache
RUN uv clean

# Enable hf-transfer
ENV HF_HUB_ENABLE_HF_TRANSFER=1
ENV VLLM_USAGE_SOURCE=kubeai

# API server entrypoint
ENTRYPOINT ["python3", "-m", "vllm.entrypoints.openai.api_server"]